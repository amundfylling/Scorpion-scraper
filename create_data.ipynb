{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from fastparquet import ParquetFile, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://th.sportscorpion.com/eng/tournament/archive/?page='\n",
    "\n",
    "# Create a set to store the URLs\n",
    "all_urls = set()\n",
    "\n",
    "# Load the existing URLs from the file if it exists\n",
    "if os.path.exists('tournament_urls.txt'):\n",
    "    with open('tournament_urls.txt', 'r') as f:\n",
    "        all_urls.update(line.strip() for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 7 new unique URLs.\n"
     ]
    }
   ],
   "source": [
    "# Start from page 1\n",
    "page_num = 1\n",
    "\n",
    "new_urls = set()\n",
    "\n",
    "while page_num < 10: # adjust this if it is a long time since the script was run\n",
    "    # Get the HTML content of the page\n",
    "    response = requests.get(base_url + str(page_num))\n",
    "\n",
    "    # If the page doesn't exist, break the loop\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table in the HTML\n",
    "    table = soup.find('table', {'class': 'sTable'})\n",
    "\n",
    "    # If there's no table on the page, break the loop\n",
    "    if table is None:\n",
    "        break\n",
    "\n",
    "    # Find all 'a' tags within the table\n",
    "    links = table.find_all('a')\n",
    "\n",
    "    # Extract the href attribute from each 'a' tag, but only if the last character is a digit\n",
    "    urls = {link.get('href') for link in links if link.get('href')[-2].isdigit()}\n",
    "\n",
    "    # Check if any of the URLs is already in the set\n",
    "    if any(url in all_urls for url in urls):\n",
    "        # If any of the URLs is already in the set, remove the urls that are in the set\n",
    "        urls = {url for url in urls if url not in all_urls}\n",
    "    \n",
    "    # Add the new URLs to the file\n",
    "    with open('tournament_urls.txt', 'a') as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "    # Add the URLs to the set\n",
    "    all_urls.update(urls)\n",
    "    new_urls.update(urls)\n",
    "    # Go to the next page\n",
    "    page_num += 1\n",
    "\n",
    "# Now new_urls contains all the new unique URLs\n",
    "print(f\"Collected {len(new_urls)} new unique URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5490"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://th.sportscorpion.com\"\n",
    "\n",
    "# Mapping of playoff stage names to fraction values\n",
    "PLAYOFF_STAGE_MAP = {\n",
    "    \"1/64 final\": 1/64,\n",
    "    \"1/32 final\": 1/32,\n",
    "    \"1/16 final\": 1/16,\n",
    "    \"1/8 final\": 1/8,\n",
    "    \"Quarterfinal\": 1/4,\n",
    "    \"Semi-final\": 1/2,\n",
    "    \"Final\": 1,\n",
    "    \"Match for the third place\": 0.9\n",
    "}\n",
    "\n",
    "def fetch_page(session, url: str) -> BeautifulSoup:\n",
    "    # Fetch the page content and return a BeautifulSoup object\n",
    "    response = session.get(url)\n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "def clean_score_text(score_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the raw score text by removing OT, W.O, etc.\n",
    "    \"\"\"\n",
    "    # A single chain of replaces is slightly more efficient than repeated calls\n",
    "    # Also more readable and consistent\n",
    "    return (score_text\n",
    "            .replace('(OT)', '')\n",
    "            .replace('(W.O)', '')\n",
    "            .replace('\\xa0', '')\n",
    "            .replace('*', '')\n",
    "            .replace('\\n', ''))\n",
    "\n",
    "def get_playoff_stage_fraction(stage_name: str) -> float:\n",
    "    # Convert the playoff stage name into a numeric fraction, e.g. 'Quarterfinal' -> 0.25\n",
    "    stage_name = stage_name.strip().lower()\n",
    "    for key, val in PLAYOFF_STAGE_MAP.items():\n",
    "        if key.lower() in stage_name:\n",
    "            return val\n",
    "    # If not found in map, default to None (unknown stage)\n",
    "    return None\n",
    "\n",
    "def extract_name_and_id(a_tag) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Given an <a> tag for a player, return (player_name, player_id).\n",
    "    If there's no <a> tag or it's missing an ID, returns (None, None).\n",
    "    \"\"\"\n",
    "    if not a_tag:\n",
    "        return None, None\n",
    "    \n",
    "    player_name = a_tag.text.strip()\n",
    "    href = a_tag.get('href', '')\n",
    "    match_id = re.search(r'/user/id/(\\d+)/', href)\n",
    "    player_id = match_id.group(1) if match_id else None\n",
    "    \n",
    "    return player_name, player_id\n",
    "\n",
    "def get_match_info(session, url: str) -> List[Tuple[str, str, str, str, str, int, int, str, str, float, int]]:\n",
    "    \"\"\"\n",
    "    Fetch match information from a given stage page.\n",
    "    \n",
    "    Returns a list of tuples:\n",
    "    (\n",
    "        URL,\n",
    "        Player1Name,\n",
    "        Player1ID,\n",
    "        Player2Name,\n",
    "        Player2ID,\n",
    "        GoalsPlayer1,\n",
    "        GoalsPlayer2,\n",
    "        Overtime,\n",
    "        Stage,            # 'Playoff' or 'Round-Robin'\n",
    "        RoundNumber,      # numeric fraction for playoff stage or round # for RR\n",
    "        PlayoffGameNumber # int or None\n",
    "    )\n",
    "    \"\"\"\n",
    "    soup = fetch_page(session, url)\n",
    "    \n",
    "    # Remove the 'saved-matches' section to avoid duplicates\n",
    "    saved_matches_div = soup.find('div', class_='saved-matches')\n",
    "    if saved_matches_div:\n",
    "        saved_matches_div.decompose()\n",
    "    \n",
    "    match_info = []\n",
    "\n",
    "    # Check if the page is for the playoff stage\n",
    "    is_playoff = len(soup.select('tr.series-container')) > 0\n",
    "\n",
    "    if is_playoff:\n",
    "        # For playoff stages, matches are organized by subheaders (Quarterfinal, Semi-final, etc.)\n",
    "        subheaders = soup.select('div.subheader')\n",
    "        for subheader in subheaders:\n",
    "            stage_name = subheader.get_text(strip=True)\n",
    "            playoff_fraction = get_playoff_stage_fraction(stage_name)\n",
    "\n",
    "            # Find the .gr_match blocks after this subheader until next subheader\n",
    "            next_siblings = subheader.find_all_next('div', class_='gr_match')\n",
    "            for block in next_siblings:\n",
    "                # If block belongs to another subheader, stop processing further blocks\n",
    "                next_sub = block.find_previous_sibling('div', class_='subheader')\n",
    "                if next_sub and next_sub != subheader:\n",
    "                    break\n",
    "\n",
    "                series = block.select('tr.series-container')\n",
    "                for serie in series:\n",
    "                    # Each player is in 'td[class^=\"ma_name\"] a'\n",
    "                    # We want the first link for player1, second link for player2\n",
    "                    players = serie.select('td[class^=\"ma_name\"] a')\n",
    "                    if len(players) < 2:\n",
    "                        continue\n",
    "                    player1_name, player1_id = extract_name_and_id(players[0])\n",
    "                    player2_name, player2_id = extract_name_and_id(players[1])\n",
    "                    \n",
    "                    # Each 'td[class^=\"ma_result_\"]' corresponds to one game in the series\n",
    "                    scores = serie.select('td[class^=\"ma_result_\"]')\n",
    "                    # Ignore the last score which is the total series score\n",
    "                    for game_number, score in enumerate(scores[:-1], start=1):\n",
    "                        if ':' in score.text:\n",
    "                            score_cleaned = clean_score_text(score.text)\n",
    "                            try:\n",
    "                                goals_player_1, goals_player_2 = map(int, score_cleaned.split(':'))\n",
    "                                overtime = 'Yes' if '(OT)' in score.text else 'No'\n",
    "                                match_info.append(\n",
    "                                    (\n",
    "                                        url,\n",
    "                                        player1_name,\n",
    "                                        player1_id,\n",
    "                                        player2_name,\n",
    "                                        player2_id,\n",
    "                                        goals_player_1,\n",
    "                                        goals_player_2,\n",
    "                                        overtime,\n",
    "                                        'Playoff',\n",
    "                                        playoff_fraction,\n",
    "                                        game_number\n",
    "                                    )\n",
    "                                )\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "\n",
    "    else:\n",
    "        # Scrape round-robin matches\n",
    "        match_tables = soup.select('table.grTable')\n",
    "        for table in match_tables:\n",
    "            header = table.select_one('th:-soup-contains(\"Tour\")')\n",
    "            if header:\n",
    "                round_text = header.get_text(strip=True)\n",
    "                round_match = re.search(r'(\\d+)\\s*Tour', round_text)\n",
    "                round_number = float(round_match.group(1)) if round_match else None\n",
    "            else:\n",
    "                round_number = None\n",
    "\n",
    "            rows = table.select('tr[id^=\"match\"]')\n",
    "            for row in rows:\n",
    "                # Player 1 link\n",
    "                player1_a = row.select_one('td.ma_name1 a')\n",
    "                player1_name, player1_id = extract_name_and_id(player1_a)\n",
    "                # If no <a>, fallback to the raw text\n",
    "                if not player1_name:\n",
    "                    player1_name = row.select_one('td.ma_name1').text.strip()\n",
    "\n",
    "                # Player 2 link\n",
    "                player2_a = row.select_one('td.ma_name2 a')\n",
    "                player2_name, player2_id = extract_name_and_id(player2_a)\n",
    "                # If no <a>, fallback to the raw text\n",
    "                if not player2_name:\n",
    "                    player2_name = row.select_one('td.ma_name2').text.strip()\n",
    "\n",
    "                score = row.select_one('td[class^=\"ma_result_\"]')\n",
    "                if score and ':' in score.text:\n",
    "                    score_cleaned = (\n",
    "                        score.text.replace('(OT)', '')\n",
    "                                  .replace('(W.O)', '')\n",
    "                                  .replace('\\xa0', '')\n",
    "                                  .replace('*', '')\n",
    "                                  .replace('\\n', '')\n",
    "                    )\n",
    "                    try:\n",
    "                        goals_player_1, goals_player_2 = map(int, score_cleaned.split(':'))\n",
    "                        overtime = 'Yes' if '(OT)' in score.text else 'No'\n",
    "                        # For round-robin, we don't have a playoff game number, so set None\n",
    "                        match_info.append(\n",
    "                            (\n",
    "                                url,\n",
    "                                player1_name,\n",
    "                                player1_id,\n",
    "                                player2_name,\n",
    "                                player2_id,\n",
    "                                goals_player_1,\n",
    "                                goals_player_2,\n",
    "                                overtime,\n",
    "                                'Round-Robin',\n",
    "                                round_number,\n",
    "                                None  # <-- None for round-robin\n",
    "                            )\n",
    "                        )\n",
    "                    except ValueError:\n",
    "                        print(f\"Unable to parse score '{score_cleaned}' from match {url}\")\n",
    "\n",
    "    return match_info\n",
    "\n",
    "def get_tournament_matches(tournament_urls: List[str], existing_stage_ids: set[str]) -> pd.DataFrame:\n",
    "    all_matches = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    def fetch_tournament_data(url):\n",
    "        with requests.Session() as session:\n",
    "            session.headers.update(headers)\n",
    "            tournament_id = url.split('/')[-2]\n",
    "            tournament_url = f\"{BASE_URL}/eng/tournament/id/{tournament_id}/\"\n",
    "            tournament_soup = fetch_page(session, tournament_url)\n",
    "\n",
    "            # Check if tournament is a team tournament, skip if yes\n",
    "            tournament_type_element = tournament_soup.select_one(\"th:-soup-contains('Tournament type') + td\")\n",
    "            tournament_type = tournament_type_element.text.strip() if tournament_type_element else 'Unknown'\n",
    "            if tournament_type.lower() == 'team':\n",
    "                return []\n",
    "\n",
    "            # Extract tournament name and date\n",
    "            tournament_name_element = tournament_soup.select_one(\"h1#header\")\n",
    "            tournament_name = tournament_name_element.text.strip() if tournament_name_element else 'Unknown'\n",
    "\n",
    "            date_element = tournament_soup.select_one(\"th:-soup-contains('Date of the tournament') + td\")\n",
    "            date = date_element.text.strip() if date_element else 'Unknown'\n",
    "\n",
    "            # Extract the stages and their sequences\n",
    "            stage_rows = tournament_soup.select('table.stages-table tr')\n",
    "            stage_data = []\n",
    "            for row in stage_rows:\n",
    "                seq_cell = row.select_one('td.stage-gr')\n",
    "                if seq_cell:\n",
    "                    stage_sequence = seq_cell.get_text(strip=True)\n",
    "                    sched_link = row.select_one('a:-soup-contains(\"Schedule and results\")')\n",
    "                    if sched_link:\n",
    "                        stage_url = f\"{BASE_URL}{sched_link['href']}?print\"\n",
    "                        stage_id = stage_url.split('/')[-3]\n",
    "                        stage_data.append((stage_id, stage_url, stage_sequence))\n",
    "\n",
    "            stage_matches = []\n",
    "            for stage_id, stage_url, stage_sequence in stage_data:\n",
    "                # Skip stage if already in existing_stage_ids\n",
    "                if stage_id in existing_stage_ids:\n",
    "                    continue\n",
    "                matches = get_match_info(session, stage_url)\n",
    "                for match in matches:\n",
    "                    # match = (\n",
    "                    #   url,\n",
    "                    #   Player1Name, Player1ID,\n",
    "                    #   Player2Name, Player2ID,\n",
    "                    #   GoalsPlayer1, GoalsPlayer2,\n",
    "                    #   Overtime, Stage, RoundNumber,\n",
    "                    #   PlayoffGameNumber\n",
    "                    # )\n",
    "                    stage_matches.append((\n",
    "                        int(stage_id) if stage_id.isdigit() else None,  # Ensure StageID is numeric\n",
    "                        match[1],  # Player1Name\n",
    "                        int(match[2]) if match[2] and match[2].isdigit() else None,  # Player1ID\n",
    "                        match[3],  # Player2Name\n",
    "                        int(match[4]) if match[4] and match[4].isdigit() else None,  # Player2ID\n",
    "                        match[5],  # GoalsPlayer1\n",
    "                        match[6],  # GoalsPlayer2\n",
    "                        match[7],  # Overtime\n",
    "                        match[8],  # Stage\n",
    "                        match[9],  # RoundNumber\n",
    "                        match[10],  # PlayoffGameNumber\n",
    "                        date,\n",
    "                        tournament_name,\n",
    "                        int(tournament_id),  # Ensure TournamentID is numeric\n",
    "                        int(stage_sequence) if stage_sequence.isdigit() else None  # Ensure StageSequence is numeric\n",
    "                    ))\n",
    "            return stage_matches\n",
    "\n",
    "    processed_tournaments = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(fetch_tournament_data, url): url for url in tournament_urls}\n",
    "        for future in tqdm(as_completed(future_to_url), total=len(future_to_url),\n",
    "                           desc=\"Processing tournaments\", unit=\"tournament\"):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                matches = future.result()\n",
    "                all_matches.extend(matches)\n",
    "                processed_tournaments += 1\n",
    "                tqdm.write(f\"\\rProcessed tournaments: {processed_tournaments}\", end='')\n",
    "            except Exception as exc:\n",
    "                print(f'{url} generated an exception: {exc}')\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        all_matches, \n",
    "        columns=[\n",
    "            'StageID',\n",
    "            'Player1',\n",
    "            'Player1ID',\n",
    "            'Player2',\n",
    "            'Player2ID',\n",
    "            'GoalsPlayer1',\n",
    "            'GoalsPlayer2',\n",
    "            'Overtime',\n",
    "            'Stage',\n",
    "            'RoundNumber',\n",
    "            'PlayoffGameNumber',\n",
    "            'Date',\n",
    "            'TournamentName',\n",
    "            'TournamentID',\n",
    "            'StageSequence'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert columns to match the schema\n",
    "    df['StageID'] = pd.to_numeric(df['StageID'], errors='coerce', downcast='integer')\n",
    "    df['Player1ID'] = pd.to_numeric(df['Player1ID'], errors='coerce', downcast='integer')\n",
    "    df['Player2ID'] = pd.to_numeric(df['Player2ID'], errors='coerce', downcast='integer')\n",
    "    df['TournamentID'] = pd.to_numeric(df['TournamentID'], errors='coerce', downcast='integer')\n",
    "    df['StageSequence'] = pd.to_numeric(df['StageSequence'], errors='coerce', downcast='integer')\n",
    "\n",
    "    # Format 'Date' to string for Parquet compatibility\n",
    "    df['Date'] = df['Date'].apply(\n",
    "        lambda x: pd.to_datetime(x, format='%d.%m.%Y', errors='coerce').strftime('%Y-%m-%d') if pd.notnull(x) else None\n",
    "    )\n",
    "\n",
    "\n",
    "    # Sort data\n",
    "    df.sort_values(by=[\"Date\", \"StageSequence\", \"RoundNumber\", \"PlayoffGameNumber\"], \n",
    "                   inplace=True, na_position='last')\n",
    "\n",
    "    # Remove playoff draws\n",
    "    df = df[~((df['Stage'] == 'Playoff') & (df['GoalsPlayer1'] == df['GoalsPlayer2']))]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1d8ed252b642439fde8c248fcc61ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tournaments:   0%|          | 0/7 [00:00<?, ?tournament/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 7"
     ]
    }
   ],
   "source": [
    "df = get_tournament_matches(list(new_urls), existing_stage_ids=set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'th_matches.parquet'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Read existing data\n",
    "    existing_df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "    # Concatenate with the new data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame back to Parquet with compression\n",
    "combined_df.to_parquet(file_path, engine='fastparquet', compression='zstd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Player table with unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('th_matches.parquet', engine='fastparquet')\n",
    "\n",
    "# Extract unique players from Player1 and Player2 columns\n",
    "player1_data = df[['Player1ID', 'Player1']].rename(columns={'Player1ID': 'PlayerID', 'Player1': 'PlayerName'})\n",
    "player2_data = df[['Player2ID', 'Player2']].rename(columns={'Player2ID': 'PlayerID', 'Player2': 'PlayerName'})\n",
    "\n",
    "# Combine both datasets\n",
    "players_data = pd.concat([player1_data, player2_data])\n",
    "\n",
    "# Drop duplicates to ensure unique PlayerID and PlayerName\n",
    "scorpion_players = players_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Ensure PlayerID is an integer\n",
    "scorpion_players['PlayerID'] = scorpion_players['PlayerID'].astype(int)\n",
    "\n",
    "# sort by PlayerID\n",
    "scorpion_players = scorpion_players.sort_values(by='PlayerID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ITHF WR ID for each player if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f52f39f567c4ab19d73cf9dcec3cf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching WR IDs:   0%|          | 0/9334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the base URL\n",
    "BASE_URL = \"https://th.sportscorpion.com\"\n",
    "\n",
    "# Function to fetch the WR ID\n",
    "def fetch_wr_id(player_id):\n",
    "    url = f\"{BASE_URL}/eng/user/id/{player_id}/\"\n",
    "    try:\n",
    "        # Use a session for persistent connections\n",
    "        with requests.Session() as session:\n",
    "            response = session.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            if response.status_code == 200:\n",
    "                # Directly parse the table containing WR ID\n",
    "                soup = BeautifulSoup(response.text, \"lxml\")\n",
    "                table = soup.find(\"table\", class_=\"iTable\")\n",
    "                if table:\n",
    "                    # Search for the WR ID using regex in the table text\n",
    "                    match = re.search(r'ID <a.*?ID=(\\d+)', str(table))\n",
    "                    if match:\n",
    "                        return match.group(1)  # Extract WR ID\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error fetching WR ID for PlayerID {player_id}: {e}\")\n",
    "    return None  # Return None if not found or an error occurs\n",
    "\n",
    "# Fetch WR IDs concurrently using ThreadPoolExecutor\n",
    "def fetch_all_wr_ids(player_ids):\n",
    "    wr_ids = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers as needed\n",
    "        # Use tqdm for a progress bar\n",
    "        results = list(tqdm(executor.map(fetch_wr_id, player_ids), total=len(player_ids), desc=\"Fetching WR IDs\"))\n",
    "        wr_ids.extend(results)\n",
    "    return wr_ids\n",
    "\n",
    "# Add the WR ID column\n",
    "scorpion_players['WR_ID'] = fetch_all_wr_ids(scorpion_players[\"PlayerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57cb7c55dd63d6a075c6d0ddb074a7b3f4af168deec5f3f8ae2a58e131ebae72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
