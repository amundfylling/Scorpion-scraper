{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 16 new unique URLs.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://th.sportscorpion.com/eng/tournament/archive/?page='\n",
    "\n",
    "# Start from page 1\n",
    "page_num = 1\n",
    "\n",
    "# Create a set to store the URLs\n",
    "all_urls = set()\n",
    "\n",
    "# Load the existing URLs from the file if it exists\n",
    "if os.path.exists('tournament_urls.txt'):\n",
    "    with open('tournament_urls.txt', 'r') as f:\n",
    "        all_urls.update(line.strip() for line in f)\n",
    "\n",
    "new_urls = set()\n",
    "\n",
    "while True:\n",
    "    # Get the HTML content of the page\n",
    "    response = requests.get(base_url + str(page_num))\n",
    "\n",
    "    # If the page doesn't exist, break the loop\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table in the HTML\n",
    "    table = soup.find('table', {'class': 'sTable'})\n",
    "\n",
    "    # If there's no table on the page, break the loop\n",
    "    if table is None:\n",
    "        break\n",
    "\n",
    "    # Find all 'a' tags within the table\n",
    "    links = table.find_all('a')\n",
    "\n",
    "    # Extract the href attribute from each 'a' tag, but only if the last character is a digit\n",
    "    urls = {link.get('href') for link in links if link.get('href')[-2].isdigit()}\n",
    "\n",
    "    # Check if any of the URLs is already in the set\n",
    "    if any(url in all_urls for url in urls):\n",
    "        # If any of the URLs is already in the set, remove the urls that are in the set\n",
    "        urls = {url for url in urls if url not in all_urls}\n",
    "    \n",
    "    # Add the new URLs to the file\n",
    "    with open('tournament_urls.txt', 'a') as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "    # Add the URLs to the set\n",
    "    all_urls.update(urls)\n",
    "    new_urls.update(urls)\n",
    "    # Go to the next page\n",
    "    page_num += 1\n",
    "\n",
    "# Now new_urls contains all the new unique URLs\n",
    "print(f\"Collected {len(new_urls)} new unique URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5237"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://th.sportscorpion.com\"\n",
    "\n",
    "def fetch_page(session, url: str) -> BeautifulSoup:\n",
    "    response = session.get(url)\n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "def get_match_info(session, url: str) -> List[Tuple[str, str, str, int, int, str, str]]:\n",
    "    soup = fetch_page(session, url)\n",
    "    match_info = []\n",
    "\n",
    "    # Check if the page is for the playoff stage\n",
    "    is_playoff = len(soup.select('tr.series-container')) > 0\n",
    "\n",
    "    if is_playoff:\n",
    "        # Scrape playoff matches\n",
    "        series = soup.select('tr.series-container')\n",
    "        for serie in series:\n",
    "            players = serie.select('td[class^=\"ma_name\"] a')\n",
    "            scores = serie.select('td[class^=\"ma_result_\"]')\n",
    "            # Ignore the last score which represents the total score of the match series\n",
    "            for score in scores[:-1]:\n",
    "                if ':' in score.text:\n",
    "                    player_1 = players[0].text.strip()\n",
    "                    player_2 = players[1].text.strip()\n",
    "                    score_cleaned = score.text.replace('(OT)', '').replace('(W.O)', '').replace('\\xa0', '').replace('*', '').replace('\\n', '')\n",
    "                    try:\n",
    "                        goals_player_1, goals_player_2 = map(int, score_cleaned.split(':'))\n",
    "                        overtime = 'Yes' if '(OT)' in score.text else 'No'\n",
    "                        match_info.append((url, player_1, player_2, goals_player_1, goals_player_2, overtime, 'Playoff'))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    else:\n",
    "        # Scrape round-robin matches\n",
    "        match_tables = soup.select('table.grTable')\n",
    "        for table in match_tables:\n",
    "            rows = table.select('tr[id^=\"match\"]')\n",
    "            for row in rows:\n",
    "                player_1 = row.select_one('td.ma_name1').text.strip()\n",
    "                player_2 = row.select_one('td.ma_name2').text.strip()\n",
    "                score = row.select_one('td[class^=\"ma_result_\"]')\n",
    "                if score and ':' in score.text:  \n",
    "                    score_cleaned = score.text.replace('(OT)', '').replace('(W.O)', '').replace('\\xa0', '').replace('*', '').replace('\\n', '')\n",
    "                    try:\n",
    "                        goals_player_1, goals_player_2 = map(int, score_cleaned.split(':'))\n",
    "                        overtime = 'Yes' if '(OT)' in score.text else 'No'\n",
    "                        match_info.append((url, player_1, player_2, goals_player_1, goals_player_2, overtime, 'Round-Robin'))\n",
    "                    except ValueError:\n",
    "                        print(f\"Unable to parse score '{score_cleaned}' from match {url}\")\n",
    "\n",
    "    return match_info\n",
    "\n",
    "# Function to scrape matches from tournaments concurrently\n",
    "def get_tournament_matches(tournament_urls: List[str], existing_stage_ids: set[str]) -> pd.DataFrame:\n",
    "    all_matches = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    def fetch_tournament_data(url):\n",
    "        with requests.Session() as session:\n",
    "            session.headers.update(headers)\n",
    "            tournament_id = url.split('/')[-2]\n",
    "            tournament_url = f\"{BASE_URL}/eng/tournament/id/{tournament_id}/\"\n",
    "            tournament_soup = fetch_page(session, tournament_url)\n",
    "            \n",
    "            tournament_name_element = tournament_soup.select_one(\"h1#header\")\n",
    "            tournament_name = tournament_name_element.text.strip() if tournament_name_element else 'Unknown'\n",
    "            \n",
    "            date_element = tournament_soup.select_one(\"th:contains('Date of the tournament') + td\")\n",
    "            date = date_element.text.strip() if date_element else 'Unknown'\n",
    "            \n",
    "            result_links = tournament_soup.select('a:contains(\"Schedule and results\")')\n",
    "            stages_urls = [f\"{BASE_URL}{link['href']}?print\" for link in result_links]\n",
    "            \n",
    "            stage_matches = []\n",
    "            for stage_url in stages_urls:\n",
    "                stage_id = stage_url.split('/')[-3]\n",
    "                if stage_id in existing_stage_ids:\n",
    "                    continue\n",
    "\n",
    "                matches = get_match_info(session, stage_url)\n",
    "                for match in matches:\n",
    "                    stage_matches.append((stage_id, *match[1:], date, tournament_name))\n",
    "            return stage_matches\n",
    "\n",
    "    processed_tournaments = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(fetch_tournament_data, url): url for url in tournament_urls}\n",
    "        for future in tqdm(as_completed(future_to_url), total=len(future_to_url), desc=\"Processing tournaments\", unit=\"tournament\"):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                matches = future.result()\n",
    "                all_matches.extend(matches)\n",
    "                processed_tournaments += 1\n",
    "                tqdm.write(f\"\\rProcessed tournaments: {processed_tournaments}\", end='')\n",
    "            except Exception as exc:\n",
    "                print(f'{url} generated an exception: {exc}')\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        all_matches, \n",
    "        columns=['StageID', 'Player1', 'Player2', 'GoalsPlayer1', 'GoalsPlayer2', 'Overtime', 'Stage', 'Date', 'TournamentName']\n",
    "    )\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y', errors='coerce')\n",
    "    df['GoalsPlayer1'] = pd.to_numeric(df['GoalsPlayer1'], errors='coerce')\n",
    "    df['GoalsPlayer2'] = pd.to_numeric(df['GoalsPlayer2'], errors='coerce')\n",
    "    df.sort_values(by=\"Date\", inplace=True)\n",
    "    df = df[~((df['Stage'] == 'Playoff') & (df['GoalsPlayer1'] == df['GoalsPlayer2']))]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:   0%|          | 0/16 [00:00<?, ?tournament/s]/Users/amundfylling/miniconda3/envs/ban438/lib/python3.10/site-packages/soupsieve/css_parser.py:876: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(\n",
      "Processing tournaments:   6%|▋         | 1/16 [00:00<00:14,  1.02tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  25%|██▌       | 4/16 [00:01<00:04,  2.91tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  31%|███▏      | 5/16 [00:01<00:02,  3.76tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  44%|████▍     | 7/16 [00:02<00:02,  3.87tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  50%|█████     | 8/16 [00:02<00:01,  4.19tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  69%|██████▉   | 11/16 [00:03<00:01,  4.95tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  75%|███████▌  | 12/16 [00:03<00:00,  4.77tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 12"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments:  88%|████████▊ | 14/16 [00:03<00:00,  3.89tournament/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tournaments: 100%|██████████| 16/16 [00:04<00:00,  3.31tournament/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tournaments: 16"
     ]
    }
   ],
   "source": [
    "df = get_tournament_matches(list(new_urls), existing_stage_ids=set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to csv\n",
    "df.to_csv('th_matches.csv', index=False, encoding='utf-8-sig', header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57cb7c55dd63d6a075c6d0ddb074a7b3f4af168deec5f3f8ae2a58e131ebae72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
